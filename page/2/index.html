<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <title>Certum ex Incertis - Certum ex Incertis</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">

<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="Certum ex Incertis">
<meta name="generator" content="Hugo 0.145.0">

    
    

  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    menuSettings: { zoom: "Double-Click" },
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  },
     "HTML-CSS" : {
         availableFonts : ["TeX"],
         preferredFont : "TeX",
         webFont : "TeX",
         imageFont : null
     }
 });
</script>

<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>







<link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.6.0/pure-min.css">


    <link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.6.0/grids-responsive-min.css">








<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">


<link rel="stylesheet" href="http://localhost:1313/css/tufte.css">
<link rel="stylesheet" href="http://localhost:1313/css/hugo-tufte.css">
<link rel="stylesheet" href="http://localhost:1313/css/hugo-tufte-override.css">

    
</head>

<body>
    <div id="layout" class="pure-g">
        <article class="pure-u-1">
            <header class="brand">
  <h1>Certum ex Incertis</h1>
  <h2>Dimitri Semenovich</h2>
  <nav class="menu">
    <ul>
    
    
        <li><a class='' href="/">Home</a></li>
    
        <li><a class='' href="/post/">Archive</a></li>
    
        <li><a class='' href="/talks/">Talks</a></li>
    
        <li><a class='' href="/papers/">Papers</a></li>
    
        <li><a class='' href="/about/">About</a></li>
    
    </ul>
</nav>

</header>

            
            
            <section style="padding-bottom:0px;">
<h1 class="content-title" style="margin-bottom:0.5rem">
  
  <a href="/2025/operators-in-machine-learning/">Operators in Machine Learning</a>
  
</h1>





<span class="content-meta">
    
    
    Mar 12, 2025&nbsp;&nbsp;
    
    
    
    <nav class="tags" style="display:inline-block; width:80%;">
        <ul >
            
            <li><a  href="http://localhost:1313/tags/functional-analysis">functional-analysis</a> </li>
            
            <li><a  href="http://localhost:1313/tags/machine-learning">machine-learning</a> </li>
            
            <li><a  href="http://localhost:1313/tags/optimization">optimization</a> </li>
            
            <li><a  href="http://localhost:1313/tags/reserving">reserving</a> </li>
            
            <li><a  href="http://localhost:1313/tags/pricing">pricing</a> </li>
            
        </ul>
    </nav>
    
</span>



</section>

            <section><p><span class="newthought"> Operator theory provides a unified perspective </span>
for understanding algorithms across domains including machine learning and actuarial science. This post explores connections between chain ladder methods, pricing models, and the functional analysis approach.</p>
<h2 id="nonlinear-functional-analysis">Nonlinear Functional Analysis</h2>
<p>Among the central topics in nonlinear functional analysis is the study of operator equations $T(x)=0$ (e.g., $Ax-b=0$, $\nabla f(x)=0$) and the associated questions:</p>
<ul>
<li>Existence of a solution</li>
<li>Uniqueness of the solution</li>
<li>Stability of the solutions under perturbation or noisy measurements</li>
<li>Construction of approximation methods and estimation of their convergence</li>
</ul>
<p>While tools of functional analysis allow for uniform treatment of widely differing problems they do not eliminate the need for detailed examinations grounded in the problem specifics.</p>
<h2 id="fixed-point-iterations">Fixed Point Iterations</h2>
<p>Fixed point iterations are the main technique for devising numerical solution methods for operator equations. Some possibilities for $T(x)=0$ include:</p>
<ul>
<li>Basic iteration: $x=(I - T)(x)$</li>
<li>Damped iteration: $x=(I - \lambda T)(x)$</li>
<li>Newton&rsquo;s method: $x=\Big(I - \big(DT(x)\big)^{-1}\circ T\Big)(x)$</li>
<li>Operator splitting: $x=F^{-1}\circ(T-G)(x)$ where $F + G = T$</li>
</ul>
<p>Key properties that guarantee convergence of these schemes and uniqueness of the solution are (local) <em>non-expansiveness</em> / <em>contractivity</em> of the iteration and <em>monotonicity</em> of $T$ respectively, the latter being essentially the operator version of convexity.</p>
<h2 id="convex-optimization">Convex Optimization</h2>
<p>Assume $f(x)=g(x)+h(x)$ is a convex function, then the subgradient $\partial f$ is a monotone operator. Some basic fixed point iterations for the operator equation $\partial f \ni 0$ include:</p>
<ul>
<li>Subgradient descent: $x\in(I-\lambda\partial f)x$</li>
<li>Proximal point method: $x=(I+\lambda\partial f)^{-1}x$ (provided $h(x)$ is smooth)</li>
<li>Proximal gradient algorithm: $x=(I+\lambda\partial g)^{-1}(I-\lambda\nabla h)x$</li>
<li>Peaceman-Rachford splitting:</li>
</ul>
<p>$$
\begin{aligned}
z &amp;= \Big(2(I+\lambda\partial g)^{-1}-I\Big)\Big(2(I+\lambda\partial h)^{-1}-I\Big)z \\
x &amp;= (I+\lambda\partial h)^{-1}z
\end{aligned}
$$</p>
<h2 id="neural-networks">Neural Networks</h2>
<p><label for="marginnote-9923c29b793d688edd8ddbb03677d0c0-1" class="margin-toggle">⊕</label>
<input type="checkbox" id="marginnote-9923c29b793d688edd8ddbb03677d0c0-1" class="margin-toggle"/>
<span class="marginnote"> <img src="/img/operator/operator_nn.png" alt=""> Neural networks can be viewed as a specific instance of operator iterations. </span></p>
<p>Consider the $\ell_1$-norm regularized least squares regression and the corresponding proximal gradient fixed point iteration:</p>
<p>$$
\underset{\mathbf{x}}{\text{minimise}}\ \ |b - Ax|_2^2 + \alpha|x|_1\  = \ f(x) + g(x)
$$</p>
<p>$$
\begin{aligned}
x^{(k+1)} &amp;= \Big(I+\frac{\alpha}{\lambda}\partial g\Big)^{-1}\Big(I - \frac{1}{\lambda}\nabla_x f\Big)\Big(x^{(k)}\Big)\\
&amp;= h_{\frac{\alpha}{\lambda}}\Big((I-\frac{1}{\lambda}A^TA)x^{(k)} + A^Tb\Big)
\end{aligned}
$$</p>
<p>where $[h_{\frac{\alpha}{\lambda}}(x)]_i=\text{sign}(x_i)\big(|x_i|-\frac{\alpha}{\lambda}\big)_{+}$</p>
<h2 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h2>
<p>MCMC algorithms to generate samples from the target distribution $p^\star$ can be viewed as constructing an ergodic Markov chain transition operator $T$ which has $p^\star$ as the stationary distribution:</p>
<p>$$
p^\star=Tp^\star
$$</p>
<p>Popular schemes such as M-H are heuristics (i.e., the <em>detailed balance</em> condition is sufficient but not necessary) to build specific instances of such operators.</p>
<p>We can arbitrarily split the operator $T$, where $T_1,\ldots,T_n$ need not be ergodic individually:</p>
<p>$$
p^\star=T_1\circ T_2\circ \ldots\circ T_np^\star
$$</p>
<p>This recovers e.g. Gibbs sampling.</p>
<p>In summary:</p>
<ul>
<li>Operator notation can help to think about a large range of machine learning problems in a unified way</li>
<li>Operator splitting in particular can be used to trivially generate custom algorithms that automatically provide certain (admittedly rather basic) guarantees</li>
<li>Functional analysis perspective helps to find related problems in other fields</li>
</ul>
</section>
            
            <section style="padding-bottom:0px;">
<h1 class="content-title" style="margin-bottom:0.5rem">
  
  <a href="/2023/basic-theory-of-entity-resolution/">Basic Theory of Entity Resolution</a>
  
</h1>





<span class="content-meta">
    
    
    Apr 10, 2023&nbsp;&nbsp;
    
    
    
    <nav class="tags" style="display:inline-block; width:80%;">
        <ul >
            
            <li><a  href="http://localhost:1313/tags/entity-resolution">entity-resolution</a> </li>
            
            <li><a  href="http://localhost:1313/tags/record-linkage">record-linkage</a> </li>
            
            <li><a  href="http://localhost:1313/tags/data-science">data-science</a> </li>
            
            <li><a  href="http://localhost:1313/tags/statistics">statistics</a> </li>
            
        </ul>
    </nav>
    
</span>



</section>

            <section><p>In this post I&rsquo;ll cover the classical record linkage formalism, key simplifying assumptions, and links to modern techniques.</p>
<h2 id="a-model-for-independent-decisions">A model for independent decisions</h2>
<p>Classical Fellegi-Sunter (1969) record linkage model is as follows — consider two sets of records $A$ and $B$, with elements denoted $a$ and $b$ respectively. Product set $A \times B$ is somehow partitioned into matches $M$ and non-matches $U$.</p>
<p>Pairs $(a,b)$ in $M$ typically agree on attributes such as first name, last name, components of date of birth and address. Pairs in $U$ may have isolated random agreements on some of these. The aim is then to recover $M$ while only having access to record attributes.</p>
<p>It should be noted that this set-up allows multiple matches between elements of $A$ and $B$ with uncertain interpretation. Further restrictions on the structure of $M$, however, require modifications to the basic &ldquo;theory&rdquo;.</p>
<p>We denote the vector of &ldquo;agreement codes&rdquo; as:</p>
<p>$$\gamma(a,b)=\big(\gamma_1(a,b),\gamma_2(a,b),\ldots,\gamma_n(a,b)\big)$$</p>
<p>where $\gamma_i(a,b)$ might be an indicator corresponding to statements like &ldquo;name is the same&rdquo;, &ldquo;name is the same and is Brown&rdquo;, &ldquo;name disagrees&rdquo;, &ldquo;name missing on one record&rdquo;, &ldquo;agreement on city part of address but not the street&rdquo;.</p>
<p>All possible realizations of agreement codes form the <em>comparison space</em> $\Gamma$, i.e. $\gamma(a,b) \in \Gamma$ for all $(a,b) \in A\times B$.</p>
<p>A randomized <em>decision rule</em> or <em>linkage rule</em> is then the mapping:</p>
<p>$$d\big(\gamma(a,b)\big): \Gamma \rightarrow \big\{ P \big( d_i,|,\gamma(a,b) \big) ,|, i=1,2,3 \big\}$$</p>
<p>which assigns a distribution, i.e. $\sum_{i=1}^3 P\big(d_i,|,\gamma(a,b)\big)=1$, over three possible decisions ${d_1,d_2,d_3}$ to each element of $\Gamma$.</p>
<p>Here $d_1$ denotes $(a,b) \in M$ (a <em>positive link</em>), $d_3$ denotes $(a,b) \in U$ (a <em>positive non-link</em>) and $d_2$ is a <em>possible link</em>.</p>
<p>To reiterate this is not the right formalism if we want to impose restrictions on the structure of match set $M$ (e.g. one to one), as in these cases decisions can no longer be taken for individual pairs $(a,b)$ in isolation.</p>
<p><label for="sidenote-65eaa79f518ad6e203a341c755f13c46-2" class="margin-toggle sidenote-number"></label>
<input type="checkbox" id="sidenote-65eaa79f518ad6e203a341c755f13c46-2" class="margin-toggle"/>
<span class="sidenote">This limitation is significant in applications like identity resolution where one-to-one matching is often required.</span></p>
<h2 id="constructing-the-optimal-decision-rule">Constructing the optimal decision rule</h2>
<p>To construct the &ldquo;optimal&rdquo; decision rule we define the probabilities of observing $\gamma$ for a match $(a,b)\in M$:</p>
<p>$$m\big(\gamma(a,b)\big)=P\big(\gamma(a,b),|,(a,b)\in M\big)$$</p>
<p>as well as a non-match $(a,b)\in U$:</p>
<p>$$u\big(\gamma(a,b)\big)=P\big(\gamma(a,b),|,(a,b)\in U\big)$$</p>
<p>It is a straightforward application of Neyman-Pearson theory to show that the &ldquo;optimal&rdquo; decision rule with respect to the usual objectives:</p>
<p>$$P(d_1,|,U)=\sum_{(a,b)\in A\times B} u\big(\gamma(a,b)\big)P(d_1 ,|,\gamma(a,b)\big) \text{ and}$$</p>
<p>$$P(d_3,|,M)=\sum_{(a,b)\in A\times B} m\big(\gamma(a,b)\big)P(d_3 ,|,\gamma(a,b)\big)$$</p>
<p>namely one that generates fewest expected false matches for a given expected number of undetected matches (as we are dealing with a bivariate objective), is a function of the likelihood ratio $\frac{m(\gamma)}{u(\gamma)}$:</p>
<p>$$d \big(\gamma(a,b)\big)= \left\{
\begin{array}{ll}
(1,0,0),&amp;T_1\le \frac{m(\gamma)}{u(\gamma)}\\
(0,1,0),&amp;T_2 &lt; \frac{m(\gamma)}{u(\gamma)} &lt;T_1\\
(0,0,1),&amp;\frac{m(\gamma)}{u(\gamma)}\le T_2
\end{array}\right.
$$</p>
<p>This is almost entirely unhelpful as we don&rsquo;t know probabilities $m$ or $u$. The original proposal of Newcombe et al. (1959) was to make some heroic independence assumptions about the structure of $P\big(\gamma(a,b),|,(a,b)\in M\big)$:</p>
<p>$$m\big(\gamma(a,b)\big)=\prod_{i=1}^n P\big(\gamma_i(a,b),|,(a,b)\in M\big)$$</p>
<p>where $\gamma_i(a,b)$ are individual components of the agreement vector and similarly for $u(\gamma)$.</p>
<p><label for="sidenote-65eaa79f518ad6e203a341c755f13c46-9" class="margin-toggle sidenote-number"></label>
<input type="checkbox" id="sidenote-65eaa79f518ad6e203a341c755f13c46-9" class="margin-toggle"/>
<span class="sidenote">The independence assumption dramatically simplifies calculation but ignores correlations between attributes that often exist in real data.</span></p>
<p>Notice that the log likelihood ratio can then be written as:</p>
<p>$$\log\left(\frac{m(\gamma)}{u(\gamma)}\right) = \sum_{i=1}^n\log\big(m_i(\gamma_i(a,b))\big)-\sum_{i=1}^n\log\big(u_i(\gamma_i(a,b))\big)$$</p>
<p>With some additional hand waving, you can convince yourself that if $\gamma_i$ is the simple agreement indicator for a certain binary attribute $\xi$, such as the presence of particular surname or a given month of birth:</p>
<p>$$\gamma_i(a,b)= \left\{
\begin{array}{ll}
1,&amp;\text{if } \xi(a)=1 \text{ and } \xi(b)=1,\\
0,&amp;\text{otherwise}
\end{array}\right.
$$</p>
<p>it might be reasonable to expect that when $\gamma_i\big((a,b)\big)=1$:</p>
<p>$$\log\big(m_i(1)\big)-\log\big(u_i(1)\big)\approx \log \big(\pi_\xi \big) - \log \big(\pi_\xi^2\big) = -\log\big(\pi_\xi\big)$$</p>
<p>where $\pi_\xi$ is the proportion of the combined population where the attribute is present. This gives us an intuitive scheme to assign weights to different matches, where e.g. rare names would be considered more informative than common ones.</p>
<h2 id="date-of-birth-distribution-patterns">Date of birth distribution patterns</h2>
<p>Here is an example what this might mean in a real world dataset:</p>



  
    <figure  class="class param">
  



  <label for="" class="margin-toggle">⊕</label>
  <input type="checkbox" id="" class="margin-toggle">
  <span class="marginnote">
  

  
  Date of birth frequencies showing lower birth rates on weekends and special dates. Notice particularly how weekends (especially Sundays) consistently show lower birth frequencies, demonstrating non-random patterns in date distributions that record linkage algorithms must account for.
  
  
  

  </span>


  
  <img src="/img/fellegi_sunter/age_distribution.png" alt="Date of birth log frequencies by day of the week">
  



</figure>

<h2 id="connection-to-modern-methods">Connection to modern methods</h2>
<p>Observe that the likelihood ratio can be approximated <em>directly</em>, i.e.:</p>
<p>$$\frac{m(\gamma)}{u(\gamma)} \approx \frac{P\big((a,b)\in M,|,\gamma\big)}{P\big((a,b)\in U,|,\gamma\big)}\frac{|U|}{|M|}$$</p>
<p>where the two components are estimated directly via probabilistic classifiers. Obvious downside of this is the hard requirement for &ldquo;training data&rdquo;.</p>
</section>
            
            


<nav class="menu" style="margin-top:5rem">
    <ul style="display: flex;
               align-items: stretch;
               justify-content: space-between;">
    
        <li>
            <a href="/">&larr; Newer Posts</a>  

    </li>
    
    <li>
        <a href="/page/3/">Older Posts &rarr;</a>
    </li>
    
</ul>
</nav>


            <footer class="page-footer">
		<hr>
		<ul class="page-footer-menu">
		
		</ul>

  

	<div class="copyright">
	<p>
    
      &copy; 2025
    .
    All rights reserved.
    
  </p>
</div>
</footer>



        </article>
    </div>
</body>
</html>
